llm_provider: "ollama"
model_name: "llama3.2-vision"
ollama_host: "http://localhost:11434"
stream: true